% $ biblatex auxiliary file $
% $ biblatex version 2.5 $
% Do not modify the above lines!
%
% This is an auxiliary file used by the 'biblatex' package.
% This file may safely be deleted. It will be recreated as
% required.
%
\begingroup
\makeatletter
\@ifundefined{ver@biblatex.sty}
  {\@latex@error
     {Missing 'biblatex' package}
     {The bibliography requires the 'biblatex' package.}
      \aftergroup\endinput}
  {}
\endgroup

\entry{espinosa2012development}{article}{}
  \name{author}{2}{}{%
    {{}%
     {Espinosa}{E.}%
     {J~Sebastian}{J.~S.}%
     {}{}%
     {}{}}%
    {{}%
     {Stryker}{S.}%
     {Michael~P}{M.~P.}%
     {}{}%
     {}{}}%
  }
  \list{publisher}{1}{%
    {Elsevier}%
  }
  \strng{namehash}{EJSSMP1}
  \strng{fullhash}{EJSSMP1}
  \field{sortinit}{E}
  \field{number}{2}
  \field{pages}{230\bibrangedash 249}
  \field{title}{Development and plasticity of the primary visual cortex}
  \field{volume}{75}
  \field{journaltitle}{Neuron}
  \field{year}{2012}
\endentry

\entry{Graves2014-ch}{article}{}
  \name{author}{3}{}{%
    {{}%
     {Graves}{G.}%
     {Alex}{A.}%
     {}{}%
     {}{}}%
    {{}%
     {Wayne}{W.}%
     {Greg}{G.}%
     {}{}%
     {}{}}%
    {{}%
     {Danihelka}{D.}%
     {Ivo}{I.}%
     {}{}%
     {}{}}%
  }
  \strng{namehash}{GAWGDI1}
  \strng{fullhash}{GAWGDI1}
  \field{sortinit}{G}
  \field{abstract}{%
  We extend the capabilities of neural networks by coupling them to external
  memory resources, which they can interact with by attentional processes. The
  combined system is analogous to a Turing Machine or Von Neumann architecture
  but is differentiable end-to-end, allowing it to be efficiently trained with
  gradient descent. Preliminary results demonstrate that Neural Turing Machines
  can infer simple algorithms such as copying, sorting, and associative recall
  from input and output examples.%
  }
  \verb{eprint}
  \verb 1410.5401
  \endverb
  \field{title}{Neural Turing Machines}
  \field{eprinttype}{arXiv}
  \field{eprintclass}{cs.NE}
  \field{month}{10}
  \field{year}{2014}
\endentry

\entry{Santoro2016-jn}{article}{}
  \name{author}{5}{}{%
    {{}%
     {Santoro}{S.}%
     {Adam}{A.}%
     {}{}%
     {}{}}%
    {{}%
     {Bartunov}{B.}%
     {Sergey}{S.}%
     {}{}%
     {}{}}%
    {{}%
     {Botvinick}{B.}%
     {Matthew}{M.}%
     {}{}%
     {}{}}%
    {{}%
     {Wierstra}{W.}%
     {Daan}{D.}%
     {}{}%
     {}{}}%
    {{}%
     {Lillicrap}{L.}%
     {Timothy}{T.}%
     {}{}%
     {}{}}%
  }
  \strng{namehash}{SA+1}
  \strng{fullhash}{SABSBMWDLT1}
  \field{sortinit}{S}
  \field{abstract}{%
  Despite recent breakthroughs in the applications of deep neural networks, one
  setting that presents a persistent challenge is that of ``one-shot
  learning.'' Traditional gradient-based networks require a lot of data to
  learn, often through extensive iterative training. When new data is
  encountered, the models must inefficiently relearn their parameters to
  adequately incorporate the new information without catastrophic interference.
  Architectures with augmented memory capacities, such as Neural Turing
  Machines (NTMs), offer the ability to quickly encode and retrieve new
  information, and hence can potentially obviate the downsides of conventional
  models. Here, we demonstrate the ability of a memory-augmented neural network
  to rapidly assimilate new data, and leverage this data to make accurate
  predictions after only a few samples. We also introduce a new method for
  accessing an external memory that focuses on memory content, unlike previous
  methods that additionally use memory location-based focusing mechanisms.%
  }
  \verb{eprint}
  \verb 1605.06065
  \endverb
  \field{title}{One-shot Learning with {Memory-Augmented} Neural Networks}
  \field{annotation}{%
  - Quite confusing...<div><br></div><div>- Input is downsampled, flattened
  images of characters w/ labels</div><div><br></div><div>- Controller is an
  lstm that uses input and state to produce 'memory key'. The key is used to
  retrieve a blend of memories that look most like the key, and is then written
  to memory at either the most recent or least-used location (is that
  determined by the controller or by a hard-learned
  parameter?)</div><div><br></div><div>- Somehow the system, and human, get
  better than chance accuracy in character classification on first
  presentation?? How is that possible? - Oh, yes: if you have already seen
  characters from many classes, then characters not looking like any of them
  likely to belong to the remaining class! Clever for the network to be able to
  see that...</div><div><br></div><div>- Also: how is performance from simple
  FF NN + kNN so terrible? Well, the FF is pretty bad - simple auto-encoder. A
  full CNN might do much better - though as they observe, it would have many
  more parameters..</div><div><br></div><div>- But the auto-encoder is trained
  unsupervised, while the MANN is trained supervised !... maybe unfair
  baseline...</div>%
  }
  \field{eprinttype}{arXiv}
  \field{eprintclass}{cs.LG}
  \field{year}{2016}
  \warn{\item Invalid format of field 'month'}
\endentry

\entry{Soltoggio2013-rg}{article}{}
  \name{author}{2}{}{%
    {{}%
     {Soltoggio}{S.}%
     {Andrea}{A.}%
     {}{}%
     {}{}}%
    {{}%
     {Steil}{S.}%
     {Jochen~J}{J.~J.}%
     {}{}%
     {}{}}%
  }
  \strng{namehash}{SASJJ1}
  \strng{fullhash}{SASJJ1}
  \field{sortinit}{S}
  \field{abstract}{%
  In the course of trial-and-error learning, the results of actions, manifested
  as rewards or punishments, occur often seconds after the actions that caused
  them. How can a reward be associated with an earlier action when the neural
  activity that caused that action is no longer present in the network? This
  problem is referred to as the distal reward problem. A recent computational
  study proposes a solution using modulated plasticity with spiking neurons and
  argues that precise firing patterns in the millisecond range are essential
  for such a solution. In contrast, the study reported in this letter shows
  that it is the rarity of correlating neural activity, and not the spike
  timing, that allows the network to solve the distal reward problem. In this
  study, rare correlations are detected in a standard rate-based computational
  model by means of a threshold-augmented Hebbian rule. The novel modulated
  plasticity rule allows a randomly connected network to learn in classical and
  instrumental conditioning scenarios with delayed rewards. The rarity of
  correlations is shown to be a pivotal factor in the learning and in handling
  various delays of the reward. This study additionally suggests the hypothesis
  that short-term synaptic plasticity may implement eligibility traces and
  thereby serve as a selection mechanism in promoting candidate synapses for
  long-term storage.%
  }
  \field{number}{4}
  \field{pages}{940\bibrangedash 978}
  \field{title}{Solving the distal reward problem with rare correlations}
  \field{volume}{25}
  \field{journaltitle}{Neural Comput.}
  \field{month}{04}
  \field{year}{2013}
\endentry

\entry{Sukhbaatar2015-ly}{incollection}{}
  \name{author}{4}{}{%
    {{}%
     {Sukhbaatar}{S.}%
     {Sainbayar}{S.}%
     {}{}%
     {}{}}%
    {{}%
     {Szlam}{S.}%
     {Arthur}{A.}%
     {}{}%
     {}{}}%
    {{}%
     {Weston}{W.}%
     {Jason}{J.}%
     {}{}%
     {}{}}%
    {{}%
     {Fergus}{F.}%
     {Rob}{R.}%
     {}{}%
     {}{}}%
  }
  \name{editor}{5}{}{%
    {{}%
     {Cortes}{C.}%
     {C}{C}%
     {}{}%
     {}{}}%
    {{}%
     {Lawrence}{L.}%
     {N~D}{N.~D.}%
     {}{}%
     {}{}}%
    {{}%
     {Lee}{L.}%
     {D~D}{D.~D.}%
     {}{}%
     {}{}}%
    {{}%
     {Sugiyama}{S.}%
     {M}{M}%
     {}{}%
     {}{}}%
    {{}%
     {Garnett}{G.}%
     {R}{R}%
     {}{}%
     {}{}}%
  }
  \list{publisher}{1}{%
    {Curran Associates, Inc.}%
  }
  \strng{namehash}{SS+1}
  \strng{fullhash}{SSSAWJFR1}
  \field{sortinit}{S}
  \field{booktitle}{Advances in Neural Information Processing Systems 28}
  \field{pages}{2440\bibrangedash 2448}
  \field{title}{{End-To-End} Memory Networks}
  \field{year}{2015}
\endentry

\entry{Vasilkoski2011-ww}{inproceedings}{}
  \name{author}{8}{}{%
    {{}%
     {Vasilkoski}{V.}%
     {Zlatko}{Z.}%
     {}{}%
     {}{}}%
    {{}%
     {Ames}{A.}%
     {Heather}{H.}%
     {}{}%
     {}{}}%
    {{}%
     {Chandler}{C.}%
     {Ben}{B.}%
     {}{}%
     {}{}}%
    {{}%
     {Gorchetchnikov}{G.}%
     {Anatoli}{A.}%
     {}{}%
     {}{}}%
    {{}%
     {L\'{e}veill\'{e}}{L.}%
     {Jasmin}{J.}%
     {}{}%
     {}{}}%
    {{}%
     {Livitz}{L.}%
     {Gennady}{G.}%
     {}{}%
     {}{}}%
    {{}%
     {Mingolla}{M.}%
     {Ennio}{E.}%
     {}{}%
     {}{}}%
    {{}%
     {Versace}{V.}%
     {Massimiliano}{M.}%
     {}{}%
     {}{}}%
  }
  \strng{namehash}{VZ+1}
  \strng{fullhash}{VZAHCBGALJLGMEVM1}
  \field{sortinit}{V}
  \field{booktitle}{The 2011 International Joint Conference on Neural Networks
  ({IJCNN})}
  \field{pages}{2563\bibrangedash 2569}
  \field{title}{Review of stability properties of neural plasticity rules for
  implementation on memristive neuromorphic hardware}
  \field{year}{2011}
\endentry

\lossort
\endlossort

\endinput
